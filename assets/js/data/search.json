[ { "title": "üî®Configuring Synology NAS to use its own domain name. Part 3", "url": "/posts/synology-configuring-domain-part-3/", "categories": "Synology, Configuration", "tags": "synology", "date": "2021-06-09 15:13:19 -0400", "snippet": "In my previous two posts ‚ÄúConfiguring Synology NAS to use own domain name. Part 1‚Äù and ‚ÄúConfiguring Synology NAS to use its own domain name. Part 2‚Äù I described configuring DSM to update Dynamic DNS in the Google Domains. Literally, until today, namely, before I updated DSM to version 7, everything worked fine. But after the update to version 7 DDNS Updater 2 stopped working due to incompatibility with the latest version. Unfortunately, I wasn‚Äôt able to get the source code of DDNS Updater 2 to have a chance to modify it and had to come up with a solution of how to live without that awesome package.For some reason, Synology still has a weird implementation of DDNS in the settings, i.e. there is no way to provide a wildcard in the domain name, due to validation error, a custom provider didn‚Äôt work for me too as well as it complains to add more than two same providers.Without hesitation, I decided to go in a different way. Since Google Domains provide an API I‚Äôve created a bash script to update the IPs and configured a scheduler to run the job each hour.#!/usr/bin/env bashWILDCARD_DOMAIN=*.fisenko.netWILDCARD_USERNAME=FROM_GOOGLE_DOMAINSWILDCARD_PASSWORD=FROM_GOOGLE_DOMAINSROOT_DOMAIN=fisenko.netROOT_USERNAME=FROM_GOOGLE_DOMAINSROOT_PASSWORD=FROM_GOOGLE_DOMAINSCURRENT_IP=$(curl -s ipecho.net/plain | cut -d '%' -f1)function update() { echo \"https://${1}:${2}@domains.google.com/nic/update?hostname=${3}&amp;myip=${CURRENT_IP}\" curl --location --request GET \"https://${1}:${2}@domains.google.com/nic/update?hostname=${3}&amp;myip=${CURRENT_IP}\"}update ${WILDCARD_USERNAME} ${WILDCARD_PASSWORD} ${WILDCARD_DOMAIN}update ${ROOT_USERNAME} ${ROOT_PASSWORD} ${ROOT_DOMAIN}Below an example of my scheduler for that.I know this is not the perfect solution since I have to modify the script manually each time I need to make a modification, but such changes are very rare it should be fine, at least for now until I have a better solution.Hopefully, you will find it helpful üôèüèªRelated Posts Configuring Synology NAS to use its own domain name. Part 1 Configuring Synology NAS to use its own domain name. Part 2" }, { "title": "üêæThe First Steps With Ubiquity", "url": "/posts/the-first-steps-with-ubiquity/", "categories": "Ubiquity", "tags": "ubiquity", "date": "2021-04-05 17:39:59 -0400", "snippet": "Not so long ago I was seriously puzzled by the question of a more competent organization of the local network at home. Since the number of devices is constantly increasing: phones, tablets, smart devices (Google Mini, Echo Spot, etc), laptops, IoT, TV, AV Receivers, etc. Security becomes the cornerstone and if all devices are connected to a single access point, in my case ASUS RT-AC3200 there is no distinction how each type of device could communicate with each other. For example, I don‚Äôt want to give smart devices like Google Mini to get access to my NAS or even ‚Äúsee‚Äù the NAS. VLANs for each type of device could be useful for delimitation the boundaries of the devices.After Googling and researching I settled upon Ubiquity products. They provide plenty of options to build the network, starting from home devices and ending with more sophisticated, enterprise-level devices.The next step was research on how to achieve what I need, i.e. delimitation the boundaries of the devices. Luckily I found an amazing YouTube channel and video on that channel, see the link below.Where the author describes how to organize, configure the VLANs, create the rules. His approach totally makes sense for my setup, even the handy spreadsheet to write down information about patch panel ports, VLANs, static IPs, comments, etc. You can download the spreadsheet from here.I made a decision to buy the initial set of Ubiquity devices but keeping in mind that I might need to extend that set, also I wanted to place all my stuff in a small rack, to keep them secure and well organized, that‚Äôs why my choice fell on the unit models. Below is my Ubiquity devices: UniFi Dream Machine Pro UniFi Switch 16 PoE UniFi 6 Lite Access Point Direct Attach Copper Cable, SFP+To place Ubiquity devices, NAS, Philips Hue Bridge, Raspberry Pi I decided to use 9 unit rack, dedicated rack mounted power strip, patch panel. Here is the list: StarTech.com 2 Post 9U 19‚Äù Wall Mount Network Cabinet Tripp Lite 14 Outlet Network-Grade Rackmount PDU StarTech.com 2U Server Rack Shelf ETS 24-Port CAT6A FTP Shielded 1U Patch Panel Ethernet cablesBelow are images of what I got after assembling.after almost one month of using the new system, I can say with confidence that it worth it, because I have more control over the system, I can better manage devices I have, I noticed bigger throughput during the day and of course, it is always fun to build something new with ability to extend in the future. So far I see the only cons is the size, definitely, it is bigger than my old ASUS RT-AC3200 üòÇ.Useful Links" }, { "title": "‚öôÔ∏è Customize Spring MongoDB DBRef Resolution", "url": "/posts/customize-spring-mongodb-dbref-resolution/", "categories": "Java, Spring", "tags": "java, spring", "date": "2021-02-12 02:00:00 -0500", "snippet": "I think I will not open a secret saying that many developers love Spring for the ability to quickly make sometimes complex things relatively simple and elegant. A huge community of developers with their questions and answers helps to quickly find a solution to a particular problem.Today I would like to share one solution that allowed me to significantly improve performance when reading data from MongoDB.In my scenario, I‚Äôm actively using DBRefs to refer from one object to another instead of embedding objects within a document. Perhaps you can argue and say that it should not be used taking into account that on the MongoDB web site there is Unless you have a compelling reason to use DBRefs, use manual references instead.But my point is if MongoDB has such type, where I can explicitly see the information about collection and ID why don‚Äôt use it. Another point is my data layer is build on top of Spring Data MongoDB which simplifies many things one thing is how Spring Data MongoDB mapping framework works with DBRefs The mapping framework does not have to store child objects embedded within the document. You can also store them separately and use a DBRef to refer to that document. When the object is loaded from MongoDB, those references are eagerly resolved so that you get back a mapped object that looks the same as if it had been stored embedded within your top-level document._ Source _: https://docs.spring.io/spring-data/mongodb/docs/3.1.3/reference/html/#mapping-usage-referencesAnd everything is good until you have relatively small objects without deep nesting. When nesting becomes quite deep or when you need to load more data or when DB grows you might notice performance degradation like happened in my scenario, i.e. each time when the mapping framework fetches the data, even if data is repeated it tries to fetch each time ignoring the fact the same portion already fetched from the DB.To resolve DBRefs by default Spring uses DefaultDbRefResolver. Enabling logging for org.springframework.data.mongodb.core.convert.DefaultDbRefResolver might be very useful to see what happens.In my application, I noticed too much Fetching DBRef... and Bulk fetching DBRefs... which pushed me to move in that direction and come up with a solution to avoid unnecessarily fetching. I decided to create a custom DbRefResolver and use Spring caching. Below is my CachedDbRefReolver@Slf4jpublic class CachedDbRefResolver extends DefaultDbRefResolver { private final Cache cache; public CachedDbRefResolver(MongoDatabaseFactory mongoDbFactory, CacheManager cacheManager) { super(mongoDbFactory); this.cache = cacheManager.getCache(CacheConstants.DBREF_CACHE_NAME); } @Override public Document fetch(@NotNull DBRef dbRef) { Document document = cache.get(dbRef.getId().toString(), Document.class); if (document == null) { document = super.fetch(dbRef); cache.put(dbRef.getId().toString(), document); } return document; } @Override public @NotNull List&lt;Document&gt; bulkFetch(List&lt;DBRef&gt; dbRefs) { List&lt;DBRef&gt; missingDbRefs = new ArrayList&lt;&gt;(); List&lt;Document&gt; result = new ArrayList&lt;&gt;(); for (DBRef dbRef : dbRefs) { Document document = cache.get(dbRef.getId().toString(), Document.class); if (document == null) { missingDbRefs.add(dbRef); } else { result.add(document); } } if (!missingDbRefs.isEmpty()) { List&lt;Document&gt; missingDocuments = super.bulkFetch(missingDbRefs); for (Document missingDocument : missingDocuments) { cache.put(missingDocument.get(MongoUtils.UNDERSCORE_ID).toString(), missingDocument); result.add(missingDocument); } } return result; }}As you can see I use DefaultDbRefResolver as the base and override two methods: fetch to fetch individual Documents bulkFetch for bulk fetching to fetch only items, not existing in the cacheIn the code above I use a few constants: CacheConstants.DBREF_CACHE_NAME is public static final String DBREF_CACHE_NAME = \"DBREF\"; MongoUtils.UNDERSCORE_ID is public static final String UNDERSCORE_ID = \"_id\";My CacheManager is ConcurrentMapCacheManager registered as@Configurationpublic class CacheBeans { @Bean public CacheManager cacheManager() { return new ConcurrentMapCacheManager(CacheConstants.DBREF_CACHE_NAME); }}See more details here Supported Cache Providers.Another important point with cache is a cache eviction. There are only two hard things in Computer Science: cache invalidation and naming things.‚Äì Phil KarltonSince I know when the data could be evicted from the cache. In my scenario, I have a custom repository to get access to the data and any update means that an item needs to be evicted. Considering all of the above I see two options: remove the documents by ID from the cache directly use @CacheEvict annotation, see docs hereFor simplicity sake, I decided to use the second option because at the moment this is more than enough for me, below is how it looks like@Slf4jpublic class MongoUserRepository implements UserRepository { @Override @CacheEvict(value = CacheConstants.DBREF_CACHE_NAME, key = \"#user.id\", beforeInvocation = true) public User update(User user) { // update user in the DB return user; }}Hopefully, this information will be useful!" }, { "title": "üï∏ Configuring Synology NAS to use its own domain name. Part 2", "url": "/posts/synology-configuring-domain-part-2/", "categories": "Synology, Configuration", "tags": "synology", "date": "2021-01-04 00:43:42 -0500", "snippet": "In my previous post, I described how to configure DDNS Updater 2 and Google Domains to use Dynamic DNS. Enough time has passed since then and I had to add many subdomains for my fisenko.net domain, each time adding a new subdomain lead me to the following steps: create a new domain configuration on the Google Domains add configuration to the DDNS Updater 2 configure the reverse proxyand each time when I need to for example rename the subdomain I had to do the same. Which is absolutely crazy and obviously not too convenient.Until today I have been postponing changing the configuration, but today I finally have found a bit of time to switch the configuration to wildcards.After the changes I did: disabled all previous DDNS Updater 2 configurations. Configurations could be removed as well if you are 100% confident.DDNS Updater 2 added a new configuration using a wildcard *.fisenko.net, see the screenshot above added a wildcard synthetic record in the Google DomainsGoogle DomainsI got a fully functional configuration without tons of subdomains in the DDNS Updater 2 and Google Domains.If you read the post then that approach works fine üòâThank you! üôèüèªRelated Posts Configuring Synology NAS to use its own domain name. Part 1 Configuring Synology NAS to use its own domain name. Part 3" }, { "title": "üìè Measure method execution time using Spring AOP", "url": "/posts/measure-method-execution-time-using-spring-aop/", "categories": "Java, Spring", "tags": "java, spring", "date": "2020-05-05 11:00:00 -0400", "snippet": "There are times when it is necessary to be able to log the execution time of a method. The simplest way to achieve that would be to modify every method by adding Stopwatch or System.currentTimeMillis() at the beginning and at the end of the method. But it leads us to the following inconvenience: the same code should be repeated many times there is no easy way to turn on and off thatBut there is a more elegant way to achieve that, keeping measuring code in one single place without the method‚Äôs body modification. And the answer is AOP. Aspect-oriented Programming (AOP) complements Object-oriented Programming (OOP) by providing another way of thinking about program structure. The key unit of modularity in OOP is the class, whereas in AOP the unit of modularity is the aspect. Aspects enable the modularization of concerns (such as transaction management) that cut across multiple types and objects. (Such concerns are often termed ‚Äúcrosscutting‚Äù concerns in AOP literature.)I‚Äôm going to use Spring AOP to measure the execution time of a method.DependencyFirst of all spring-boot-starter-aop dependency needs to be added. Below is an example of the Maven configuration.&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt;AnnotationThe next step is to create a custom annotation that will be used to annotate a method that needs to be measured.@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Measured { String message() default \"\";}There is a message in the annotation to pass a custom message to the log.Aspect ClassThe next step is to create an aspect that will use the Measured annotation above.@Log4j2@Aspect@Configuration@EnableAspectJAutoProxypublic class MeasuredAspect { @Around(\"@annotation(net.fisenko.annotations.Measured)\") public Object measureExecutionTime(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); Object proceed = joinPoint.proceed(); long executionTime = System.currentTimeMillis() - start; MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature(); Method method = methodSignature.getMethod(); Measured measured = method.getAnnotation(Measured.class); String message = measured.message(); if (Strings.isNullOrEmpty(message)) log.debug(\"Method {} execution: {} ms\", joinPoint.getSignature().toShortString(), executionTime); else log.debug(\"{}: {} ms\", message, executionTime); return proceed; }}What this code does: @Aspect annotation declares that this class is an aspect @Around ¬†annotation declares that the advice will be run before and after the target method @annotation(net.fisenko.annotations.Measured) is a pointcut, which means that all methods annotated with @Measured will be associated with this advice The advice itself simply measures time before and after target method execution joinPoint.proceed() and logsAnnotation UsageHaving the annotation and the aspect class the measurement becomes as simple as possible. For example:@RestController@RequestMapping(\"/fibonacci\")public class FibonacciController { private final FibonacciService fibonacciService; public FibonacciController(FibonacciService fibonacciService) { this.fibonacciService = fibonacciService; } @GetMapping @Measured(message = \"Get Fibonacci number\") public ResponseEntity&lt;Long&gt; getFibonacciNumber(@PathVariable int n) { return ResponseEntity.ok(fibonacciService.getFibonacciNumber(n)); }}Links https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#aop https://martinfowler.com/articles/domain-oriented-observability.html#Aspect-orientedProgramming https://en.wikipedia.org/wiki/Aspect-oriented_programming" }, { "title": "üõ† Creating a Linux service with systemd", "url": "/posts/creating-a-linux-service-with-systemd/", "categories": "Linux", "tags": "linux, iot", "date": "2019-12-01 19:46:48 -0500", "snippet": "Oftentimes writing application there is a need to offload compute-heavy tasks to an asynchronous worker script, schedule tasks for later, or even write a daemon that listens to a socket to communicate with clients.The cool thing is that it‚Äôs fairly easy to create a Linux service: use your favorite programming language to write a long-running program, and turn it into a service using systemd. systemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system. systemd provides aggressive parallelization capabilities, uses socket and D-Bus activation for starting services, offers on-demand starting of daemons, keeps track of processes using Linux control groups, maintains mount and automount points, and implements an elaborate transactional dependency-based service control logic. systemd supports SysV and LSB init scripts and works as a replacement for sysvinit. Other parts include a logging daemon, utilities to control basic system configuration like the hostname, date, locale, maintain a list of logged-in users and running containers and virtual machines, system accounts, runtime directories and settings, and daemons to manage simple network configuration, network time synchronization, log forwarding, and name resolution.The last time when I need to run such a service it was a Python script to read temperature and humidity data from the XBee receiver connected to Raspberry Pi via USB.To create a SERVICE_NAME service with systemd the following steps would be done: create a unit file, e.g. SERVICE_NAME.service, like[Unit]Description=Service Descrioption[Service]WorkingDirectory=/home/&lt;user&gt;/&lt;directory&gt;ExecStart=&lt;command to run&gt;Restart=alwaysRestartSec=10SyslogIdentifier=&lt;identifier&gt;User=&lt;user&gt;[Install]WantedBy=multi-user.targetNote : A real unit file example for Home Temperature Monitoring could be found here. copy SERVICE_NAME.service to /etc/systemd/system/ make sure the script is executable chmod +x /etc/systemd/system/SERVICE_NAME.service enable the service sudo systemctl enable SERVICE_NAME.service start the service sudo systemctl start SERVICE_NAME.service check the service‚Äôs status sudo systemctl status SERVICE_NAME.serviceIn my case I‚Äôve used the minimum systemd features, more details about unit files could be found here https://www.freedesktop.org/software/systemd/man/systemd.unit.html" }, { "title": "‚õì Convert event-based API to Flux", "url": "/posts/convert-event-based-listeners-to-flux/", "categories": "Java, Reactor", "tags": "java, flux", "date": "2019-11-16 11:00:00 -0500", "snippet": "When working with reactive streams in Java, you may encounter a situation when there is an event-based API that must somehow return data as a stream.In my case for Reactive Streams, I use Reactor. Let‚Äôs say we have a simple listener object with onUpdate callback method, like in the code snippet below.public class CustomListener implements BaseListener { private final FluxConnector&lt;String[]&gt; namesFluxConnector;; public MarketDataListener() { namesFluxConnector = new FluxConnector&lt;&gt;(); } @Override public void onUpdate(String[] names) { namesFluxConnector.next(names); } public Flux&lt;String[]&gt; getFlux() { return namesFluxConnector.getFlux(); }}And we need to work with data as with stream, in the code above CustomListener returns Flux&lt;String[]&gt; object to consume the stream later. The main transformations are done in FluxConnector. FluxConnector creates a Flux and uses FluxSink to push objects into stream. A simple implementation to convey the main idea in the code snippet below.public class FluxConnector&lt;T&gt; { private final Flux&lt;T&gt; flux; private FluxSink&lt;T&gt; fluxSink; public FluxConnector() { this.flux = Flux .create(new SinkAdapter&lt;T&gt;(this::setFluxSink)) .share(); } public void next(T data) { if (Objects.nonNull(fluxSink)) fluxSink.next(data); } public Flux&lt;T&gt; getFlux() { return flux; } private void setFluxSink(FluxSink&lt;T&gt; fluxSink) { if (Objects.isNull(this.fluxSink)) this.fluxSink = fluxSink; } private static class SinkAdapter&lt;T&gt; implements Consumer&lt;FluxSink&lt;T&gt;&gt; { private final Consumer&lt;FluxSink&lt;T&gt;&gt; consumer; SinkAdapter(Consumer&lt;FluxSink&lt;T&gt;&gt; consumer) { this.consumer = consumer; } @Override public void accept(FluxSink&lt;T&gt; sink) { consumer.accept(sink); } }}Links Project Reactor Reactive Streams Specification Reactive Streams" }, { "title": "üèóÔ∏è Docker container update automation on Synology NAS", "url": "/posts/docker-container-update-automation-on-synology-nas/", "categories": "Synology, Deployment", "tags": "synology, docker", "date": "2019-11-15 00:20:00 -0500", "snippet": "Using Docker on Synology NAS is quite straightforward. Deploying a new container comes down to a few simple steps: download an image, launch with required parameters and that is it. Updating also straightforward, but every time the same steps need to be done. Especially tiresome when a container has a set of environment variables and volume mappings and especially when there are bunch of such containers.Fortunately, there is a simple and elegant way of how to automate the containers update process using Watchtower. Watchtower is an application that will monitor your running Docker containers and watch for changes to the images that those containers were originally started from. If watchtower detects that an image has changed, it will automatically restart the container using the new image. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially.The most important thing is the ability to run Watchtower as a Docker container on NAS. The Docker image for Watchtower is available on the Docker Hub and has comprehensive documentation.To run the Watchtower on the NAS there is on a small trick than needs to be done, point is that the path to docker.sock should be passed as a volume setting, but Synology Docker package doesn‚Äôt allow to configure such path. As a workaround I did the following steps: Enabled SSH connections Connected to the NAS via SSH Ran the Watchtower on the NAS, like sudo docker run -d --name watchtower -v /var/run/docker.sock:/var/run/docker.sock v2tec/watchtower Disabled SSH connectionsAfter those simple steps, the Watchtower works and Synology Docker package shows it." }, { "title": "üîå Adding RAM to Synology DS 218+", "url": "/posts/adding-ram-to-synology-ds-218/", "categories": "Synology, Tuning", "tags": "synology", "date": "2019-11-09 19:58:52 -0500", "snippet": "Recently I‚Äôve faced the limitation of 2GB of RAM when I wanted to run GitLab Docker container on Synology DS 218+. By default, GitLab recommends having at least 8GB to run the container. But Synology DS 218+ has only 2 GB. The same limitation I would face if I need multiple containers even if they are small the total amount of memory required to keep them up and running exceeds 2 GB.Fortunately, it‚Äôs possible to extract, replace, and upgrade the RAM inside the Synology DS218+. The process only requires a few steps.In my case, I extended the 2GB of system memory all the way up to 10GB.Below are the steps of how to upgrade Synology DS 218+ RAM Shutdown the NAS. Take out the two hard drive bays. The free RAM module slot is located on the side where the power button is. Take out the newly purchased RAM module from its packaging. I used Crucial 8GB Single DDR3/DDR3L 1600 MT/S (PC3-12800). Carefully insert the module into the slot, making sure it‚Äôs the right way up. Match the notch to that on the module. Re-insert the hard drive bays. Turn on the NAS.Note : some guides recommend to use Synology Assistant to check the memory, I didn‚Äôt do that luckily, everything works without any problemsAfter those simple steps, you will be able to take full advantage of 10GB of RAM. The official word from Synology is only a 4GB module can be installed (for a total of 6GB) but 10GB seems to work just fine.Resource MonitorIf you want to go all out and hit 16GB of RAM, it‚Äôs possible to replace the original 2GB module, but you need to take apart the NAS completely and isn‚Äôt recommended unless you truly need extra memory." }, { "title": "üöÄ Deploying Ghost to Synology NAS", "url": "/posts/deploying-ghost-in-docker/", "categories": "Synology, Deployment", "tags": "synology, docker, mariadb", "date": "2018-12-04 03:02:54 -0500", "snippet": "Previously I described how to configure Synology NAS to use own domain name. In this post, I‚Äôm going to describe how to deploy Ghost in Synology NAS using Docker. Ghost is a free and open-source blogging platform written in JavaScript and distributed under the MIT License, designed to simplify the process of online publishing for individual bloggers as well as online publications.Docker is a computer program that performs operating-system-level virtualization, also known as ‚Äúcontainerization‚Äù. It was first released in 2013 and is developed by Docker, Inc. Docker is used to running software packages called ‚Äúcontainers‚Äù.All the steps below are performed on Synology DS218+First of all, we need to install the Docker package on the NAS. It doesn‚Äôt require a special configuration, just go to the Package Center, search Docker, and install it. That is it.Docker PackageThen I‚Äôd strongly recommend installing MariaDB10 package. Of course, Ghost could use SQLiteas the DB, but I‚Äôm going to use MariaDB.MariaDB 10 PackageThere is a phpMyAdminpackage which helps manage installed MariaDB databases, users, and other stuff related to the database. So, I‚Äôd recommend installing that package as well.I‚Äôve created a new blog database in phpMyAdmin as well as a separate user for the database.phpMyAdminAfter that, we are good to go to deploy Ghost in the Docker.In the Docker package in the Registry, search and download ghost imageafter that, the image will be available in the Images, where the ghost image should be selected and launched, as on the screenshot belowphpMyAdminLaunch Ghost imageBelow I‚Äôll provide the settings for the container I usedGeneral SettingsI use Low CPU Priority setting along with 256Mb Memory LimitAdvanced SettingsVolume SettingsPrior the Volume configuration docker/personal/ghost should be created on the storage, ¬†it will allow the docker container write data to the storage through /var/ghost/content directory, i.e. save changes like a theme, images when the container is restarted.Port SettingAny unused NAS local port could be used for the Port Settings. The most important part is Environment, see the values I used to make it works.Environment SettingsAfter applying these steps Ghost container should be up and running.Running DockerAnd available locally e.g. by local IP address http://192.168.1.2:8080. To make the blog available outside and have a domain name like https://blog.fisenko.net (also see Configuring Synology NAS to use own domain name go to the Control Panel, Application Portal, Reverse Proxy and create a configuration for HTTP and HTTPS protocols:HTTP ConfigurationHTTP ConfigurationAfter these manipulations, the blog will be accessible from the Internet.Hopefully, you will find these instructions useful. Thank you! üôèUpdates09/07/2020: *.fisenko.page domains on all screenshots could be interpreted as *.fisenko.net domains due to migration to the .net domain." }, { "title": "‚òÅÔ∏è Configuring Synology NAS to use its own domain name. Part 1", "url": "/posts/synology-configuring-domain-part-1/", "categories": "Synology, Configuration", "tags": "synology", "date": "2018-11-22 04:33:24 -0500", "snippet": "In this post, I describe how you could configure Synology NAS to use your own domain even if you don‚Äôt have static IP.Synology NAS provides a build-in functionality to configure your domain like fisenko.net even you don‚Äôt have a static IP address. You just need to open: Control Panel, External Access, and click on Add in DDNS section.It works fine until you need to add sub-domains, like blog.fisenko.net, blahblahblah.fisenko.page etc. For some reason, the build-in DDNS settings don‚Äôt allow to add more than one domain for one service provider. For example, I want to configure fisenko.net, blog.fisenko.net with Google service provide.Fortunately, there is awesome package DDNS Updater 2 DDNS Updater 2 is a web interface and client to configure and automatically update dynamic DNS hostnames. A variety of providers can be created ‚ÄòOut of the Box‚Äô by the customizable user dialogues, configurable response codes, and protocols. If a protocol still can not set up API compliant, this can be achieved through an additional module. The setup wizard is another new tool with multiple features. These range from the first analysis via functional tests of compatibility with a module up to the completion of a new provider or protocol. New providers or protocols, if the compatibility is provided with a built-in module, can be distributed by import/export function easily via the Community.To get it worked DDNS Updater 2 needs to be downloaded and installed after that multiple sub-domains could be configured as belowDDNS Updater 2DDNS Updater 2 domain configurationThe same configuration needs to be done for all subdomains.In my case, I use Google Domains since it provides Dynamic DNS support out of the box. More details about Dynamic DNS could be found on the Google Domains Help.Now DNS is configured but an additional step is required to add HTTPS support. Fortunately, Synology already has built-in Let‚Äôs Encrypt support. Certificates could be added as shown below:After these simple steps domains will be available over HTTPS with a valid certificate, e.g. blog.fisenko.net and fisenko.net.Update: read Configuring Synology NAS to use its own domain name. Part 2 to see how to simplify the configuration.Thank you! üôèüèªRelated Posts Configuring Synology NAS to use its own domain name. Part 2 Configuring Synology NAS to use its own domain name. Part 3Links Synology 2 bay NAS DiskStation DS218+ WD Red 4TB NAS Hard Drive" }, { "title": "üñ• Home Temperature Monitoring", "url": "/posts/home-temperature-monitoring/", "categories": "IoT, Arduino", "tags": "arduino", "date": "2018-11-02 13:07:00 -0400", "snippet": "The story is about how I built a homebrew temperature/humidity monitoring system using Arduino, XBee, and Raspberry Pi.Part 1. AbstractThe main idea is to build a system to monitor temperature and humidity in my house and have access from a phone, tablet, laptop, etc. Additionally, I want to have historical data and show temperature, and humidity charts for the last 24 hours, month, and year, for instance.To build the monitoring system I‚Äôm going to use Arduino Pro Mini as the main device which will read data from DHT22 temperature/humidity sensor and send them by Digi ZigBee Series 1. As a receiving device, I will use Raspberry Pi 3 with connected Digi ZigBee Series 1 but configured as the receiver.All code is available on GitHub, feel free to fork and use the code.Let‚Äôs get started.Part 2. Building Sending DeviceIn this section I‚Äôm going to go through the steps I did to create a device that will get temperature, and humidity data from the DHT22 sensor and send it to a receiver, see Part 3. Building Receiving Device section.The final device looks as in the images below:Part 2.1. Configuring XBeeEach XBee device must be configured to communicate with another XBee device, also please note that here I use XBee Series 1, see some details in the following posts XBee Series 1 vs. Series 2, XBee Buying Guide.For the configuration, I used XCTU - which allows you to configure XBee modules and see received data sent by other XBee modules if you would like to debug how XBee module configuration works. To connect the XBee module via USB I used Waveshare XBee USB Adapter.You could follow the following steps to set up XBee modules: In XCTU click on Add device to add USB connected XBee moduleXCTU Main Screen Select the connected device, as on the screenshot below.Add module dialog Note: if you don‚Äôt see your XBee module, make sure you have installed a driver.If you use Windows you could see the connected device in the Device Manager. Ihad such an issue and after installing that particular driver, the issue hasbeen resolved.After that, you will see the connected XBee module in the list of modulesConnected XBee3.¬†Select the module and update: ID, DL, and MY values.Here is what I used for my configuration: Device ID PAN ID DL Destination Address Low MY 16-bit Source Address Receiver 3400 3501 3500 Sender 1 3400 3500 3501 Sender 2 3400 3500 3502 Sender 3 3400 3500 3503 PAN ID must be the same for all devices, DL for Sender [x]‚Äôs is 3500 as theReceiver‚Äôs MY 16-bit Source Address value. Senders MY 16-bit Source Address justunique IDs.After these steps, XBee modules are ready for the next steps.Useful links: Exploring XBees and XCTU XBee WiFi Hookup GuidePart 2.2. Configuring ArduinoDHT22 and Digi ZigBee Series 1 need to be connected to Arduino Pro Mini before uploading the code. See the schema below how I connected Arduino Pro Mini with DHT22 and Digi ZigBee Series 1:SketchArduino Pro Mini pinout is available here.I used an additional Breakout Board for XBee Module with Generic 2mm 10 Pin XBee Socket Header to connect Digi ZigBee Series 1.When everything is connected code can be uploaded to the Arduino Pro Mini. Arduino Pro Mini doesn‚Äôt have a USB port you need to use FTDI device, I used SparkFun FTDI Basic Breakout - 3.3V.Please note: that I used 3.3V Arduino Pro Mini and therefore you have to use 3.3V FTDI.Arduino sketch file is available here - Monitoring.Sender.ino, you just need to provide a unique DEVICE_ID for your device, e.g. GUID in line #4 of the sketch file and upload to Arduino Pro Mini.Probably when you upload the code into your Arduino you will need to disconnect the XBee module, otherwise, you will get an error in the Arduino IDE.Part 3. Building Receiving DevicePart 3.1. Configuring Raspberry PiBefore running any code on Raspberry Pi, you need to connect Digi ZigBee Series 1. In my case, I used the XBee USB Adapter for it.Now you are ready to upload the code into your Raspberry Pi. The easiest way is to use SSH. In this post, I‚Äôm not going to describe how to install Raspbian and configure SSH, but you could easily google it.Software for the Raspberry Pi(receiver) consist from two parts: A Python scriptwhich reads data from the USB connected XBee module and calls Web API to send received data; A Web part consists from an API and UI the API: receives requests from the Python script and save into MariaDB retrieve data for the UI SPA application the UI is the Angular application: shows connected devices and theirs temperature, humidity, and head index; shows historical data for each particular device using charts To build and deploy the code to the Raspberry Pi use scripts from the scripts directory: build.sh to build API and UI, deploy.sh to deploy(don‚Äôt forget to update the IP address in the deploy.sh script).You could call sh ./build.sh &amp;&amp; sh ./deploy.sh from the scripts directory to build and deploy.Prerequisites to run the code on the Raspberry Pi: Python 3 must be installed; .NET Core must be install, see Link #4 in the Links section.After deployment, the code connects to your Raspberry Pi, navigates to the monitoring directory and call nohup sh ./run.sh &amp; to leave run.sh the script running when you close SSH connection.Part 4. ResultThe final result is a Web Application that looks likeMain screen.Feel free to ask me any questions.Links Low Power Arduino! Deep Sleep Tutorial ATMEL 8-BIT MICROCONTROLLER WITH 4/8/16/32KBYTES. IN-SYSTEM PROGRAMMABLE FLASH DATASHEET Setting up Raspian and .NET Core 2.0 on a Raspberry Pi .NET Core on Raspberry Pi" } ]
